# IMPORTANT PREREQUISITES:
# 1. Make sure Ollama is running on your system:
#    - Check with: `ollama serve`
#    - If you see "address already in use", Ollama is already running
#    - If not running, start with: `ollama serve`
# 
# 2. Before using a model, pull it first:
#    - Example: `ollama pull mistral`
#    - Available models: https://ollama.ai/library
#    - Other examples: 
#      * ollama pull llama2
#      * ollama pull codellama
#      * ollama pull phi

# Model Configuration
model:
  name: "gemma:7b"  # The model to use (must be pulled via ollama first!)
  temperature: 0.7
  max_tokens: 1000
  top_p: 0.9

# Prompt Configuration
prompt:
  path: "conversation_detection_prompt.txt"  # Path to the prompt file

# Output Configuration
output:
  format: "csv"  # Output format (csv, json)
  include_confidence: true  # Whether to include confidence scores
  timestamp_format: "%Y%m%d_%H%M%S"  # Format for timestamp in output filenames
  output_dir: "data/groups"  # Base directory for output files

# Processing Configuration
processing:
  batch_size: 10  # Number of messages to process in each batch
  max_context_messages: 5  # Number of previous messages to include for context
  min_confidence_threshold: 0.7  # Minimum confidence score to accept a prediction 
